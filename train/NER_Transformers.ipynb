{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "628d12a9",
   "metadata": {},
   "source": [
    "Тип используемой разметки\n",
    "\n",
    "- *B-{label}* - начало сущности *{label}*;\n",
    "- *I-{label}* - продолжение сущности *{label}*;\n",
    "- *O* - отсутсвие сущности.\n",
    "\n",
    "Здесь в качестве сущности *{label}* может выступать имя, географическое название или какой-то другой тип собственных имён.\n",
    "\n",
    "Например, мы хотим извлечь имена и названия организаций. Тогда для текста\n",
    "\n",
    "    Yan    Goodfellow  works  for  Google Corp Brain\n",
    "\n",
    "модель должна извлечь следующую последовательность:\n",
    "\n",
    "    B-PER  I-PER       O      O    B-ORG  I-ORG I-ORG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dc7800b-b83b-474f-a831-f6da06e05d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import torch.utils\n",
    "import numpy as np\n",
    "from tqdm import tqdm,trange\n",
    "import random\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5066ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "valid_df = pd.read_csv('data/valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8e9f2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sent, train_label = [i.split() for i in train_df['sent']],[i.split() for i in train_df['lable']]\n",
    "test_sent, test_label = [i.split() for i in test_df['sent']],[i.split() for i in test_df['lable']]\n",
    "valid_sent, valid_label = [i.split() for i in valid_df['sent']],[i.split() for i in valid_df['lable']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bdaaaa",
   "metadata": {},
   "source": [
    "Устанавливаем один и тот же сид для воспроизводимости результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac28858f-4b53-406f-8cf8-13726d391de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_global_seed(seed:int)->None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark=False\n",
    "    torch.backends.cudnn.determnistic = True\n",
    "set_global_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc44e65f",
   "metadata": {},
   "source": [
    "Проинициализируем device (CPU / GPU) на котором будем работать:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f373adc-b552-47be-a51c-49e2844b6448",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59524dd4",
   "metadata": {},
   "source": [
    "Подготовка словаря лейблов\n",
    "\n",
    "{**label**}→{**label_idx**}: соответствие между тегом и уникальным индексом (начинается с 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66555615-bd0b-4bf8-ba47-052505d00226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label2idx(label_seq):\n",
    "    label2idx = {}\n",
    "    label_list = set(label for sentence in label_seq for label in sentence)\n",
    "    label_list = sorted(label_list, key = lambda x: 'A' if x=='O' else x)\n",
    "    for i, label in enumerate(label_list):\n",
    "        label2idx[label] = i\n",
    "    return label2idx\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12c6c3e3-45e3-4067-bb82-314f5a1b2518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-LOC': 1,\n",
       " 'B-MISC': 2,\n",
       " 'B-ORG': 3,\n",
       " 'B-PER': 4,\n",
       " 'I-LOC': 5,\n",
       " 'I-MISC': 6,\n",
       " 'I-ORG': 7,\n",
       " 'I-PER': 8}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2idx = get_label2idx(train_label)\n",
    "label2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "775647a8-5c75-4f9e-81ef-caca05db4953",
   "metadata": {},
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def process_labels(labels,label2idx):\n",
    "    label_ids = list()\n",
    "    label_ids = [label2idx[label] for label in labels]\n",
    "    return label_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e28c61",
   "metadata": {},
   "source": [
    "Загрузка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d46adfcd-aa3d-41e1-b9da-d0662b6fe018",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[0;32m      2\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-cased\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_name = 'bert-base-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d11ffe",
   "metadata": {},
   "source": [
    "### Подготовка датасета и загрузчика\n",
    "\n",
    "Мы также хотим обучать модель батчами, поэтому нам как и прежде понадобятся `Dataset`, `Collator` и `DataLoader`.\n",
    "\n",
    "Но мы не можем переиспользовать те, что в предыдущих частях задания, так как обработка данных должна производится немного иначе с использованием токенизатора.\n",
    "\n",
    "Давайте напишем новый кастомный датасет, который на вход (метод `__init__`) будет принимать:\n",
    "- token_seq - список списков слов / токенов\n",
    "- label_seq - список списков тегов\n",
    "\n",
    "и возвращать из метода `__getitem__` два списка:\n",
    "- список текстовых значений (`List[str]`) из индексов токенов в сэмпле\n",
    "- список целочисленных значений (`List[int]`) из индексов соответвующих тегов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc59e9b0-dde7-4821-a175-5fed8dab1570",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, token_seq, label_seq):\n",
    "        self.token_seq = token_seq\n",
    "        self.label_seq = [self.process_labels(labels,label2idx) for labels in label_seq]\n",
    "    def __len__(self):\n",
    "        return len(self.token_seq)\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.token_seq[idx]\n",
    "        labels = self.label_seq[idx]\n",
    "        return tokens,labels\n",
    "    @staticmethod\n",
    "    def process_labels(labels,label2idx):\n",
    "        ids = [label2idx[i] for i in labels]\n",
    "        return ids        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b9db4c5-216b-4a3f-9c69-4a7ea94cdf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TransformeDataset(token_seq=train_sent, label_seq=train_label)\n",
    "valid_dataset = TransformeDataset(token_seq=valid_sent, label_seq=valid_label)\n",
    "test_dataset = TransformeDataset(token_seq=test_sent, label_seq=test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a832f629",
   "metadata": {},
   "source": [
    "Реализуем новый `Collator`.\n",
    "\n",
    "Инициализировать коллатор будет 3 аргументами:\n",
    "- токенизатор\n",
    "- параметры токенизатора в виде словаря (затем используем как `**kwargs`)\n",
    "- id спецтокена для последовательностей тегов (значение -1)\n",
    "\n",
    "Метод `__call__` на вход принимает батч, а именно список кортежей того, что нам возвращается из датасета. В нашем случае это список кортежей двух int64 тензоров - `List[Tuple[torch.LongTensor, torch.LongTensor]]`.\n",
    "\n",
    "На выходе мы хотим получить два тензора:\n",
    "- западденные индексы слов / токенов\n",
    "- западденные индексы тегов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a91cd75a-ec5d-4b74-9c69-4ffa495e6224",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer\n",
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "\n",
    "class TransformesCollator:\n",
    "    def __init__(self, tokenizer,tokenizer_kwargs,label_padding_value):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer_kwargs = tokenizer_kwargs\n",
    "        self.label_padding_value = label_padding_value\n",
    "    def __call__(self, batch):\n",
    "        tokens,labels = zip(*batch)\n",
    "        tokens = self.tokenizer(list(tokens),**self.tokenizer_kwargs)\n",
    "        labels =self.encode_labels(tokens,labels,self.label_padding_value)\n",
    "        tokens.pop('offset_mapping')\n",
    "        return tokens,labels\n",
    "    @staticmethod\n",
    "    def encode_labels(tokens, labels,label_padding_value):\n",
    "        encoded_labels = []\n",
    "        for doc_labels, doc_offset in zip(labels, tokens.offset_mapping):\n",
    "            doc_enc_labels = np.ones(len(doc_offset), dtype=int)*label_padding_value\n",
    "            arr_offset = np.array(doc_offset)\n",
    "            \n",
    "            doc_enc_labels[(arr_offset[: ,0] ==0) & (arr_offset[: ,1]!=0)]=doc_labels\n",
    "            encoded_labels.append(doc_enc_labels.tolist())\n",
    "        return torch.LongTensor(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f65ee47a-328f-4929-9350-675da6fa27bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_kwargs = {\n",
    "    'is_split_into_words': True,\n",
    "    'return_offsets_mapping':True,\n",
    "    'padding': True,\n",
    "    'truncation': True,\n",
    "    'max_length': 512,\n",
    "    'return_tensors': 'pt',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee8bc0e1-6bd0-4208-ba5f-3170cc4c1239",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = TransformesCollator(tokenizer =tokenizer, tokenizer_kwargs=tokenizer_kwargs,label_padding_value=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "979a362b-166f-41d4-ab04-1d57370a86dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=5, shuffle=True,collate_fn=collator)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=1, shuffle=False, shuffle=False, collate_fn=collator,)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False,collate_fn=collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8602e910",
   "metadata": {},
   "source": [
    "В библиотеке **transformers** есть классы для модели BERT, уже настроенные под решение конкретных задач, с соответствующими головами классификации. Для задачи NER будем использовать класс `BertForTokenClassification`.\n",
    "\n",
    "По аналогии с токенизаторами, мы можем использовать класс `AutoModelForTokenClassification`, который по названию модели сам определит, какой класс нужен для инициализации модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60363d45-cda7-481d-bd43-617c8f88e20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels = len(label2idx)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a344346-6193-43ca-b9af-f7c2311d27f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-4,eps=1e-8)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d02c302e-e0ee-4f2f-bf2c-9b327348198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(log_dir=f'logs/Transformers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e30b715",
   "metadata": {},
   "source": [
    "Функция подсчета метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1c6c50b-99e1-4c26-b28c-70f668f3ece1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(outputs, labels):\n",
    "    metrics = {}\n",
    "    mask = (labels!=-1)\n",
    "    y_true = labels[mask].cpu().numpy()\n",
    "    y_pred = outputs.argmax(1)\n",
    "    y_pred = y_pred[mask].cpu().numpy()\n",
    "    metrics['accuracy'] = accuracy_score(y_true = y_true, y_pred = y_pred)\n",
    "    for metric_func in [precision_score, recall_score, f1_score]:\n",
    "        metric_name = metric_func.__name__.split('_')[0]\n",
    "        for average_type in ['micro','macro','weighted']:\n",
    "            metrics[metric_name+'_'+average_type] = metric_func( y_true=y_true, y_pred=y_pred,average=average_type,zero_division =0)\n",
    "    return metrics\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9220de8",
   "metadata": {},
   "source": [
    "Функция обучения (получение аутпутов, подсчет лоссов и метрик)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "abbc35bf-1fd0-4125-ba65-eaeb8ed80377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model,dataloader,optimizer,criterion,writer,device,epoch):\n",
    "    model.train()\n",
    "    epoch_loss = []\n",
    "    batch_metrics_list = defaultdict(list)\n",
    "    for i, (tokens,labels) in tqdm(enumerate(dataloader),total=len(dataloader),desc='loop over train batches'):\n",
    "        tokens, labels = tokens.to(device),labels.to(device)\n",
    "        outputs=None\n",
    "        loss = None\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**tokens)['logits'].transpose(1,2)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss.append(loss.item())\n",
    "        writer.add_scalar( tag = 'batch loss/train', scalar_value = loss.item(), global_step = epoch*len(dataloader) + i)\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            outputs_inference = model(**tokens)['logits'].transpose(1,2)\n",
    "        batch_metrics = compute_metrics(outputs = outputs_inference, labels=labels)\n",
    "        for metric_name, metric_value in batch_metrics.items():\n",
    "            batch_metrics_list[metric_name].append(metric_value)\n",
    "            writer.add_scalar(tag = f'batch {metric_name}/train', scalar_value = metric_value, global_step=epoch*len(dataloader) + i)\n",
    "    avg_loss = np.mean(epoch_loss)\n",
    "    print(f'Train loss: {avg_loss}\\n')\n",
    "    for metric_name, metric_value_list in batch_metrics_list.items():\n",
    "        metric_value = np.mean(metric_value_list)\n",
    "        print(f'Train {metric_name}: {metric_value}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bb4651",
   "metadata": {},
   "source": [
    "Функция проверки точности на тестовом датасете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_epoch(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    criterion: torch.nn.Module,\n",
    "    writer: SummaryWriter,\n",
    "    device: torch.device,\n",
    "    epoch: int,\n",
    ") :\n",
    "    model.eval()\n",
    "    epoch_loss = []\n",
    "    batch_metrics_list = defaultdict(list)\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, (tokens, labels) in tqdm(enumerate(dataloader),total=len(dataloader),desc=\"loop over test batches\",):\n",
    "\n",
    "            tokens, labels = tokens.to(device), labels.to(device)\n",
    "            outputs = model(**tokens)[\"logits\"].transpose(1, 2)\n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_loss.append(loss.item())\n",
    "            writer.add_scalar(tag=\"batch loss / test\",scalar_value=loss.item(),global_step=epoch * len(dataloader) + i,)\n",
    "            batch_metrics = compute_metrics(\n",
    "                outputs=outputs,\n",
    "                labels=labels,\n",
    "            )\n",
    "\n",
    "            for metric_name, metric_value in batch_metrics.items():\n",
    "                batch_metrics_list[metric_name].append(metric_value)\n",
    "                writer.add_scalar(\n",
    "                    tag=f\"batch {metric_name} / test\",\n",
    "                    scalar_value=metric_value,\n",
    "                    global_step=epoch * len(dataloader) + i,\n",
    "                )\n",
    "        avg_loss = np.mean(epoch_loss)\n",
    "        print(f\"Test loss:  {avg_loss}\\n\")\n",
    "        writer.add_scalar(tag=\"loss / test\",scalar_value=avg_loss,global_step=epoch,)\n",
    "        for metric_name, metric_value_list in batch_metrics_list.items():\n",
    "            metric_value = np.mean(metric_value_list)\n",
    "            print(f\"Test {metric_name}: {metric_value}\\n\")\n",
    "            writer.add_scalar(\n",
    "                tag=f\"{metric_name} / test\",\n",
    "                scalar_value=np.mean(metric_value),\n",
    "                global_step=epoch,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "27883145-d71a-4062-b33e-d76d24ceb668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, model, train_dataloader, valid_dataloader,optimizer,criterion,writer,device):\n",
    "    for epoch in range(n_epochs):\n",
    "        train_epoch(model=model ,dataloader = train_dataloader, optimizer=optimizer, criterion=criterion, writer=writer, device=device, epoch=epoch)\n",
    "        evaluate_epoch(model=model ,dataloader = valid_dataloader, optimizer=optimizer, criterion=criterion, writer=writer, device=device, epoch=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5402bf97-8d82-4384-b11c-1987453dbc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loop over train batches: 100%|██████████| 3/3 [00:11<00:00,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4289773851633072\n",
      "\n",
      "Train accuracy: 0.9029791761665907\n",
      "\n",
      "Train precision_micro: 0.9029791761665907\n",
      "\n",
      "Train precision_macro: 0.6177871148459383\n",
      "\n",
      "Train precision_weighted: 0.8464564527741482\n",
      "\n",
      "Train recall_micro: 0.9029791761665907\n",
      "\n",
      "Train recall_macro: 0.5333333333333333\n",
      "\n",
      "Train recall_weighted: 0.9029791761665907\n",
      "\n",
      "Train f1_micro: 0.9029791761665907\n",
      "\n",
      "Train f1_macro: 0.5298592271055647\n",
      "\n",
      "Train f1_weighted: 0.8621313709902693\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train(n_epochs=1,model=model,train_dataloader=train_dataloader,optimizer=optimizer,criterion=criterion,writer=writer,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "033797fa-4048-4f87-a791-9e60b5a033d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loop over test batches: 100%|██████████| 5/5 [00:00<00:00,  8.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.19298928380012512\n",
      "\n",
      "Test accuracy: 0.8873737373737374\n",
      "\n",
      "Test precision_micro: 0.8873737373737374\n",
      "\n",
      "Test precision_macro: 0.4436868686868687\n",
      "\n",
      "Test precision_weighted: 0.7875886389143965\n",
      "\n",
      "Test recall_micro: 0.8873737373737374\n",
      "\n",
      "Test recall_macro: 0.5\n",
      "\n",
      "Test recall_weighted: 0.8873737373737374\n",
      "\n",
      "Test f1_micro: 0.8873737373737374\n",
      "\n",
      "Test f1_macro: 0.470140056022409\n",
      "\n",
      "Test f1_weighted: 0.8344673627026568\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_epoch(model=model,\n",
    "    dataloader=test_dataloader,\n",
    "    criterion=criterion,\n",
    "    writer=writer,\n",
    "    device=device,\n",
    "    epoch=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
